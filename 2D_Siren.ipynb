{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2D_Siren.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0nOZGKGP1R6"
      },
      "source": [
        "# Install Packages on Google Colab and import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvt4dd2SVHFp",
        "outputId": "9ad7ed24-8f86-4908-b36b-92f95746291c"
      },
      "source": [
        "!pip install vtk\n",
        "!pip install pyevtk\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pdb\n",
        "import csv\n",
        "from torch.utils.data import DataLoader, TensorDataset,RandomSampler\n",
        "from math import exp, sqrt,pi\n",
        "import time\n",
        "import vtk\n",
        "from vtk.util import numpy_support as VN\n",
        "import math\n",
        "import os.path\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.workbook import Workbook\n",
        "from pyevtk.hl import pointsToVTK"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vtk\n",
            "  Downloading vtk-9.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (88.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 88.3 MB 48 kB/s \n",
            "\u001b[?25hCollecting wslink>=1.0.4\n",
            "  Downloading wslink-1.1.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from vtk) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0.0->vtk) (1.15.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->wslink>=1.0.4->vtk) (2.0.7)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 48.4 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 44.9 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 43.1 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->wslink>=1.0.4->vtk) (21.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->wslink>=1.0.4->vtk) (3.10.0.2)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->wslink>=1.0.4->vtk) (2.10)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, aiohttp, wslink, vtk\n",
            "Successfully installed aiohttp-3.8.0 aiosignal-1.2.0 async-timeout-4.0.0 asynctest-0.13.0 frozenlist-1.2.0 multidict-5.2.0 vtk-9.1.0 wslink-1.1.0 yarl-1.7.2\n",
            "Collecting pyevtk\n",
            "  Downloading pyevtk-1.4.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from pyevtk) (1.19.5)\n",
            "Installing collected packages: pyevtk\n",
            "Successfully installed pyevtk-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_H2QGvfP8Rc"
      },
      "source": [
        "# Connect to google drive to load the files and write the outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhCo3EueVL3Y",
        "outputId": "8d68c641-b225-4398-e6ed-aa88d7779ae5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gHwcmcQGXw"
      },
      "source": [
        "# The Principal function that contains Siren-Based Network, training, and output writing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r-cI9g6VRRY"
      },
      "source": [
        "# Two Helper function\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def cast_tuple(val, repeat = 1):\n",
        "    return val if isinstance(val, tuple) else ((val,) * repeat)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-5mtXJEVPKQ"
      },
      "source": [
        "# choose that you want to run the program on CPU or GPU\n",
        "Flag_GPU = 1\n",
        "if (Flag_GPU==1):\n",
        "\tdevice = torch.device(\"cuda\") \n",
        "else:\n",
        "\tdevice = torch.device(\"cpu\")\n",
        "\n",
        "def geo_train(device,x_in,y_in,xb,yb,ub,vb,xd,yd,ud,vd,batchsize,learning_rate,epochs,path,Flag_batch,Diff,rho,Flag_BC_exact,Lambda_BC,nPt,T,xb_inlet,yb_inlet,ub_inlet,vb_inlet ):\n",
        "\tx_asli= x_in\n",
        "\ty_asli= y_in\n",
        "\tif (Flag_batch):\n",
        "\t x = torch.Tensor(x_in).to(device)\n",
        "\t y = torch.Tensor(y_in).to(device)\n",
        "\t xb = torch.Tensor(xb).to(device)\n",
        "\t yb = torch.Tensor(yb).to(device)\n",
        "\t ub = torch.Tensor(ub).to(device)\n",
        "\t vb = torch.Tensor(vb).to(device)\n",
        "\t xd = torch.Tensor(xd).to(device)\n",
        "\t yd = torch.Tensor(yd).to(device)\n",
        "\t ud = torch.Tensor(ud).to(device)\n",
        "\t vd = torch.Tensor(vd).to(device)\n",
        "\t xb_inlet = torch.Tensor(xb_inlet).to(device)\n",
        "\t yb_inlet = torch.Tensor(yb_inlet).to(device)\n",
        "\t ub_inlet = torch.Tensor(ub_inlet).to(device)\n",
        "\t vb_inlet = torch.Tensor(vb_inlet).to(device)\n",
        "\t if(Flag_GPU==1): #Cuda slower in double?   For Float this means that it only has four decimal places while Double still has twelve\n",
        "\t\t\t\t\t# Therefore, if we change the double to fload the usage of the memory will be lower, so network will be trained with a better pace using GPU\n",
        "\t\t x = x.type(torch.cuda.FloatTensor)\n",
        "\t\t y = y.type(torch.cuda.FloatTensor)\n",
        "\t\t xb = xb.type(torch.cuda.FloatTensor)\n",
        "\t\t yb = yb.type(torch.cuda.FloatTensor)\n",
        "\t\t ub = ub.type(torch.cuda.FloatTensor)\n",
        "\t\t vb = vb.type(torch.cuda.FloatTensor)\n",
        "\t\t xb_inlet = xb_inlet.type(torch.cuda.FloatTensor)\n",
        "\t\t yb_inlet = yb_inlet.type(torch.cuda.FloatTensor)\n",
        "\t\t ub_inlet = ub_inlet.type(torch.cuda.FloatTensor)\n",
        "\t\t vb_inlet = vb_inlet.type(torch.cuda.FloatTensor)\n",
        "\t\t xd = xd.type(torch.cuda.FloatTensor)\n",
        "\t\t yd = yd.type(torch.cuda.FloatTensor)\n",
        "\t\t ud = ud.type(torch.cuda.FloatTensor)\n",
        "\t\t vd = vd.type(torch.cuda.FloatTensor)\n",
        "\n",
        "\t # generate the input dataset (it was 2D)\n",
        "\t dataset = TensorDataset(x,y)\n",
        "\t dataloader = DataLoader(dataset, batch_size=batchsize,shuffle=True,num_workers = 0, drop_last = True )\n",
        "\n",
        "\telse:\n",
        "\t x = torch.Tensor(x_in).to(device)\n",
        "\t y = torch.Tensor(y_in).to(device) \n",
        "\n",
        "\th_n = 128\t\t\t\t\t #Width for u,v,p # number of hidden neurons\n",
        "\tinput_n = 2 \t\t\t # this is what our answer is a function of (x,y)\n",
        "\n",
        "\tclass Sine(nn.Module):\n",
        "\t\tdef __init__(self, w0 = 1.):\n",
        "\t\t\tsuper().__init__()\n",
        "\t\t\tself.w0 = w0\n",
        "\t\tdef forward(self, x):\n",
        "\t\t\treturn torch.sin(self.w0 * x)\n",
        "\n",
        "\tclass Siren(nn.Module):\n",
        "\t\tdef __init__(self, dim_in, dim_out, w0 = 1., c = 6., is_first = False, use_bias = True, activation = None):\n",
        "\t\t\tsuper().__init__()\n",
        "\t\t\tself.dim_in = dim_in\n",
        "\t\t\tself.is_first = is_first\n",
        "\t\t\tif (Flag_GPU==1):\n",
        "\t\t\t\tweight = torch.zeros(dim_out, dim_in , device=\"cuda\")\n",
        "\t\t\telse:\n",
        "\t\t\t\tweight = torch.zeros(dim_out, dim_in)\n",
        "\t\t\t\n",
        "\t\t\tif (Flag_GPU==1):\n",
        "\t\t\t\tbias = torch.zeros(dim_out, device=\"cuda\") if use_bias else None\n",
        "\t\t\telse:\n",
        "\t\t\t\tbias = torch.zeros(dim_out) if use_bias else None\n",
        "\n",
        "\t\t\tself.init_(weight, bias, c = c, w0 = w0)\n",
        "\n",
        "\t\t\tself.weight = nn.Parameter(weight)\n",
        "\t\t\tself.bias = nn.Parameter(bias) if use_bias else None\n",
        "\t\t\tself.activation = Sine(w0) if activation is None else activation\n",
        "\n",
        "\t\tdef init_(self, weight, bias, c, w0):\n",
        "\t\t\tdim = self.dim_in\n",
        "\n",
        "\t\t\tw_std = (1 / dim) if self.is_first else (math.sqrt(c / dim) / w0)\n",
        "\t\t\tweight.uniform_(-w_std, w_std)\n",
        "\n",
        "\n",
        "\t\t\tif exists(bias):\n",
        "\t\t\t\tbias.uniform_(-w_std, w_std)\n",
        "\n",
        "\t\tdef forward(self, x):\n",
        "\t\t\tout =  F.linear(x, self.weight, self.bias)\n",
        "\t\t\tout = self.activation(out)\n",
        "\t\t\treturn out\n",
        "\n",
        "\tclass SirenNet(nn.Module):\n",
        "\t\tdef __init__(self, dim_in, dim_hidden, dim_out, num_layers, w0 = 1., w0_initial = 30., use_bias = True, final_activation = None):\n",
        "\t\t\tsuper().__init__()\n",
        "\t\t\tself.num_layers = num_layers\n",
        "\t\t\tself.dim_hidden = dim_hidden\n",
        "\n",
        "\t\t\tself.layers = nn.ModuleList([])\n",
        "\t\t\tfor ind in range(num_layers):\n",
        "\t\t\t\tis_first = ind == 0\n",
        "\t\t\t\tlayer_w0 = w0_initial if is_first else w0\n",
        "\t\t\t\tlayer_dim_in = dim_in if is_first else dim_hidden\n",
        "\n",
        "\t\t\t\tself.layers.append(Siren(\n",
        "\t\t\t\t\t\tdim_in = layer_dim_in,\n",
        "\t\t\t\t\t\tdim_out = dim_hidden,\n",
        "\t\t\t\t\t\tw0 = layer_w0,\n",
        "\t\t\t\t\t\tuse_bias = use_bias,\n",
        "\t\t\t\t\t\tis_first = is_first\n",
        "\t\t\t\t))\n",
        "\n",
        "\t\t\tfinal_activation = nn.Identity() if not exists(final_activation) else final_activation\n",
        "\t\t\tself.last_layer = Siren(dim_in = dim_hidden, dim_out = dim_out, w0 = w0, use_bias = use_bias, activation = final_activation)\n",
        "\n",
        "\n",
        "\t\tdef forward(self, x, mods = None):\n",
        "\t\t\tx=x*2 -1\n",
        "\t\t\tmods = cast_tuple(mods, self.num_layers)\n",
        "\t \n",
        "\t\t\tfor layer, mod in zip(self.layers, mods):\n",
        "\t\t\t\tx = layer(x)\n",
        "\n",
        "\t\t\t\tif exists(mod):\n",
        "\t\t\t\t\tx *= rearrange(mod, 'd -> () d')\n",
        "\n",
        "\t\t\treturn self.last_layer(x)\n",
        "\n",
        "\n",
        "\tnet2 = net = SirenNet(\n",
        "\t\tdim_in = 2,                        # input dimension, ex. 2d coor\n",
        "\t\tdim_hidden = 128,                  # hidden dimension\n",
        "\t\tdim_out = 3,                       # output dimension, ex. rgb value\n",
        "\t\tnum_layers = 4,                    # number of layers\n",
        "\t\tw0_initial = 15.                   # different signals may require different omega_0 in the first layer - this is a hyperparameter\n",
        ")\n",
        "  \n",
        "\n",
        "### Optimizers of Networks\n",
        "\toptimizer_Siren = optim.Adam(net2.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
        "\t\n",
        "\n",
        "###### Define Losses - We have 3 losses containing: Loss of the equations, loss of the sensor Data, Loss of BC (no slip in wall boundries)\n",
        "\tdef criterion(x,y):\n",
        "\n",
        "\t\tx.requires_grad = True\n",
        "\t\ty.requires_grad = True\n",
        "\t\tnet_in = torch.cat((x,y),1)\n",
        "\t\toutnet = net2(net_in)\n",
        "\t\tu= outnet[:,0]\n",
        "\t\tv= outnet[:,1]\n",
        "\t\tP= outnet[:,2]\n",
        "\t\tu = u.view(len(u),-1)\n",
        "\t\tv = v.view(len(v),-1)\n",
        "\t\tP = P.view(len(P),-1)\n",
        "\t\t# prepare the gradient to make the equations as losses\n",
        "\t\tu_x = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
        "\t\tu_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
        "\t\tu_y = torch.autograd.grad(u,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
        "\t\tu_yy = torch.autograd.grad(u_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
        "\t\tv_x = torch.autograd.grad(v,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
        "\t\tv_xx = torch.autograd.grad(v_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
        "\t\tv_y = torch.autograd.grad(v,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
        "\t\tv_yy = torch.autograd.grad(v_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
        "\t\tP_x = torch.autograd.grad(P,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
        "\t\tP_y = torch.autograd.grad(P,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
        "\t\t# scale to magnitude the losses ( not necessary )\t\t\n",
        "\t\tXX_scale = U_scale * (X_scale**2)\n",
        "\t\tYY_scale = U_scale * (Y_scale**2)\n",
        "\t\tUU_scale  = U_scale **2\n",
        "\t\n",
        "\t\tloss_2 = u*u_x / X_scale + v*u_y / Y_scale - Diff*( u_xx/XX_scale  + u_yy /YY_scale  )+ 1/rho* (P_x / (X_scale*UU_scale)   )  #X-dir\n",
        "\t\tloss_1 = u*v_x / X_scale + v*v_y / Y_scale - Diff*( v_xx/ XX_scale + v_yy / YY_scale )+ 1/rho*(P_y / (Y_scale*UU_scale)   ) #Y-dir\n",
        "\t\tloss_3 = (u_x / X_scale + v_y / Y_scale) #continuity\n",
        "\n",
        "\t\t# MSE LOSS\n",
        "\t\tloss_f = nn.MSELoss()\n",
        "\n",
        "\t\t#Note our target is zero. It is residual so we use zeros_like\n",
        "\t\tloss = loss_f(loss_1,torch.zeros_like(loss_1))+  loss_f(loss_2,torch.zeros_like(loss_2))+  loss_f(loss_3,torch.zeros_like(loss_3))\n",
        "\n",
        "\t\treturn loss\n",
        "\n",
        "\tdef Loss_BC(xb,yb,ub,vb, xb_inlet, yb_inlet, ub_inlet, x, y ):\n",
        "\t\tnet_in1 = torch.cat((xb, yb), 1)\n",
        "\t\toutnet = net2(net_in1)\n",
        "\t\tout1_u = outnet[:,0]\n",
        "\t\tout1_v = outnet[:,1]\n",
        "\t\tout1_u = out1_u.view(len(out1_u),-1)\n",
        "\t\tout1_v = out1_v.view(len(out1_v),-1)\n",
        "\n",
        "\t\tloss_f = nn.MSELoss()\n",
        "\t\tloss_noslip = loss_f(out1_u, torch.zeros_like(out1_u)) + loss_f(out1_v, torch.zeros_like(out1_v)) \n",
        "\n",
        "\t\treturn loss_noslip\n",
        "\n",
        "\n",
        "\tdef Loss_data(xd,yd,ud,vd ):\n",
        "\t\tnet_in1 = torch.cat((xd, yd), 1)\n",
        "\t\toutnet = net2(net_in1)\n",
        "\t\tout1_u = outnet[:,0]\n",
        "\t\tout1_v = outnet[:,1]\n",
        "\t\tout1_u = out1_u.view(len(out1_u),-1)\n",
        "\t\tout1_v = out1_v.view(len(out1_v),-1)\n",
        "\n",
        "\t\tloss_f = nn.MSELoss()\n",
        "\t\tloss_d = loss_f(out1_u, ud) + loss_f(out1_v, vd) \n",
        "\n",
        "\t\treturn loss_d\n",
        "\n",
        "\t# Main loop\n",
        "\n",
        "\ttic = time.time()\n",
        " \n",
        "\tFlag_pretrain = True # True #If true reads the nets from last run\n",
        "\tif(Flag_pretrain & os.path.isfile(path+\"sten_data\" + \".pt\")):\n",
        "\t\tprint('Reading (pretrain) functions first...')\n",
        "\t\tnet2.load_state_dict(torch.load(path+\"sten_data\" + \".pt\"))\n",
        "\n",
        "\t\t\n",
        "\tif (Flag_schedule):\n",
        "\t\tscheduler_u = torch.optim.lr_scheduler.StepLR(optimizer_Siren, step_size=step_epoch, gamma=decay_rate)\n",
        "\n",
        "\tif(Flag_batch):# This one uses dataloader\n",
        "\t\t\tLOSS = np.zeros(epochs)\n",
        "\t\t\tLOSS_eq = np.zeros(epochs)\n",
        "\t\t\tLOSS_data = np.zeros(epochs)\n",
        "\t\t\tLOSS_BC = np.zeros(epochs)\n",
        "\t\t\tzaman = np.zeros(epochs)\n",
        "\n",
        "\t\t\tfor epoch in range(epochs):\n",
        "\n",
        "\t\t\t\tloss_eqn_tot = 0.\n",
        "\t\t\t\tloss_bc_tot = 0.\n",
        "\t\t\t\tloss_data_tot = 0.\n",
        "\t\t\t\tn = 0\n",
        "\t\t\t\tfor batch_idx, (x_in,y_in) in enumerate(dataloader): \n",
        "\t\t\t\t\tnet2.zero_grad()\n",
        "\t\t\t\t\tloss_eqn = criterion(x_in,y_in)\n",
        "\t\t\t\t\tloss_bc = Loss_BC(xb,yb,ub,vb,xb_inlet,yb_inlet,ub_inlet,x,y)\n",
        "\t\t\t\t\tloss_data = Loss_data(xd,yd,ud,vd)\n",
        "\t\t\t\t\tloss = loss_eqn + Lambda_BC* loss_bc + Lambda_data*loss_data\n",
        "\n",
        "\t\t\t\t\tloss.backward()\n",
        "\t\t\t\t\toptimizer_Siren.step() \n",
        "\t\t\t\t\tloss_eqn_tot += loss_eqn\n",
        "\t\t\t\t\tloss_bc_tot += loss_bc\n",
        "\t\t\t\t\tloss_data_tot  += loss_data\n",
        "\t\t\t\t\tn += 1 \n",
        "\t\t\t\t\tif batch_idx % 40 ==0:\n",
        "\t\t\t\t\t\tprint('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss eqn {:.10f} Loss BC {:.8f} Loss data {:.8f}'.format(\n",
        "\t\t\t\t\t\t\tepoch, batch_idx * len(x_in), len(dataloader.dataset),\n",
        "\t\t\t\t\t\t\t100. * batch_idx / len(dataloader), loss_eqn.item(), loss_bc.item(),loss_data.item()))\n",
        "\n",
        "\t\t\t\tif (Flag_schedule):\n",
        "\t\t\t\t\t\tscheduler_u.step()\n",
        "\n",
        "\t\t\t\tloss_eqn_tot = loss_eqn_tot / n\n",
        "\t\t\t\tloss_bc_tot = loss_bc_tot / n\n",
        "\t\t\t\tloss_data_tot = loss_data_tot / n\n",
        "\t\t\t\tloss_avg_tot = loss_eqn_tot + Lambda_BC* loss_bc_tot + Lambda_data*loss_data_tot\n",
        "\t\t\t\twb = Workbook()\n",
        "\t\t\t\tworkbook_name = path+'loss_siren.xlsx'\n",
        "\t\t\t\twb = load_workbook(workbook_name)\n",
        "\t\t\t\tpage = wb.active\n",
        "\t\t\t\tinformation = [[loss_eqn_tot.cpu().detach().numpy(),loss_bc_tot.cpu().detach().numpy(),loss_data_tot.cpu().detach().numpy(),loss_avg_tot.cpu().detach().numpy()]]\n",
        "\t\t\t\tinformation=np.array(information)\n",
        "\t\t\t\tfor info in information.tolist():\n",
        "\t\t\t\t\tpage.append(info)\n",
        "\t\t\t\twb.save(filename=workbook_name)\n",
        "\t\t\t\ttorch.save(net2.state_dict(),path+\"sten_data\" + \".pt\")\n",
        "\n",
        "\t\t\t\tprint('*****Total avg Loss : Loss eqn {:.10f} Loss BC {:.10f} Loss data {:.10f} ****'.format(loss_eqn_tot, loss_bc_tot,loss_data_tot) )\n",
        "\t\t\t\tprint('learning rate is ', optimizer_Siren.param_groups[0]['lr'])\n",
        "\t\t\n",
        "\t\t\tif(0): #This causes out of memory in cuda in autodiff\n",
        "\t\t\t\tloss_eqn = criterion(x,y)\t\n",
        "\t\t\t\tloss_bc = Loss_BC(xb,yb,ub,vb)\n",
        "\t\t\t\tloss = loss_eqn #+ Lambda_BC* loss_bc\n",
        "\t\t\t\tprint('**** Final (all batches) \\tLoss: {:.10f} \\t Loss BC {:.6f}'.format(\n",
        "\t\t\t\t\tloss.item(),loss_bc.item()))\n",
        "\n",
        "\telse:\n",
        "\t\tfor epoch in range(epochs):\n",
        "\t\t\t##Closure function for LBFGS loop:\n",
        "\t\t\tnet2.zero_grad()\n",
        "\t\t\tloss_eqn = criterion(x,y)\n",
        "\t\t\tloss_bc = Loss_BC(xb,yb,cb)\n",
        "\t\t\tif (Flag_BC_exact):\n",
        "\t\t\t\tloss = loss_eqn #+ loss_bc\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss = loss_eqn + Lambda_BC * loss_bc\n",
        "\t\t\tloss.backward()\n",
        "\n",
        "\t\t\toptimizer_Siren.step() \n",
        "\n",
        "\t\t\tif epoch % 10 ==0:\n",
        "\t\t\t\tprint('Train Epoch: {} \\tLoss: {:.10f} \\t Loss BC {:.6f}'.format(\n",
        "\t\t\t\t\tepoch, loss.item(),loss_bc.item()))\n",
        "\n",
        "\ttoc = time.time()\n",
        "\telapseTime = toc - tic\n",
        "\tprint (\"elapse time in parallel = \", elapseTime)\n",
        "\t###################\n",
        "\t#plot\n",
        "\tif(1):#save network\n",
        "\t\ttorch.save(net2.state_dict(),path+\"sten_data\" + \".pt\")\n",
        "\n",
        "\n",
        "\t\tprint (\"Data saved!\")\n",
        "\n",
        "\n",
        "\t\n",
        "\tnet_in = torch.cat((x.requires_grad_(),y.requires_grad_()),1)\n",
        "\toutneti = net2(net_in)  #evaluate model (runs out of memory for large GPU problems!)\n",
        "\toutput_u = outneti[:,0]\n",
        "\toutput_v = outneti[:,1]  #evaluate model\n",
        "\toutput_p = outneti[:,2]\n",
        "\n",
        "\n",
        "\toutput_u = output_u.cpu().data.numpy() #need to convert to cpu before converting to numpy\n",
        "\toutput_v = output_v.cpu().data.numpy()\n",
        "\toutput_p = output_p.cpu().data.numpy()\n",
        "\tx = x.cpu()\n",
        "\ty = y.cpu()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tplt.figure()\n",
        "\tplt.subplot(2, 1, 1)\n",
        "\tplt.scatter(x.detach().numpy(), y.detach().numpy(), c = output_u , cmap = 'rainbow')\n",
        "\tplt.title('NN results, u')\n",
        "\tplt.colorbar()\n",
        "\tplt.show()\n",
        "\tplt.figure()\n",
        "\tplt.subplot(2, 1, 1)\n",
        "\tplt.scatter(x.detach().numpy(), y.detach().numpy(), c = output_v , cmap = 'rainbow')\n",
        "\tplt.title('NN results, v')\n",
        "\tplt.colorbar()\n",
        "\tplt.show()\n",
        " \n",
        "\tnpoints = len(x)\n",
        "\tz = np.zeros((npoints,1))\n",
        "\tu = output_u\n",
        "\tv = output_v\n",
        "\tvel_z=np.zeros((npoints,1))\n",
        "\tp = output_p\n",
        "\tv=np.float64(v)\n",
        "\tu=np.float64(u)\n",
        "\tpointsToVTK(path+\"./output_siren\", x_asli, y_asli, z, data = {\"velocity\" : (u,v,vel_z), \"pressure\" : p})\n",
        "\tprint('vtu file is constructed')\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\treturn net2\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LupVP12CxWmb"
      },
      "source": [
        "# Main code (Input data and preprocessing - define hyper parameteres - run the functions/classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbF_IeIY__B-"
      },
      "source": [
        "## load the mesh file & boundry files. Then, extract and prepare the coordinates of them\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brsStToUVjJs",
        "outputId": "af2f8f94-79a9-491c-d898-9b9a4ef54a4f"
      },
      "source": [
        "# Open the raw Data from google Drive\n",
        "Directory = \"/content/drive/My Drive/stenosis/\"   # the address that I've saved my data there, on Google Colab\n",
        "mesh_file = Directory + \"sten_mesh000000.vtu\"\n",
        "bc_file_in = Directory + \"inlet_BC.vtk\"\n",
        "bc_file_wall = Directory + \"wall_BC.vtk\"\n",
        "File_data = Directory + \"velocity_sten_steady.vtu\"\n",
        "fieldname = 'f_5-0' #The velocity field name in the vtk file (see from ParaView)\n",
        "\n",
        "print ('\\nLoading the mesh file and make x & y coordinates ready in variables x & y')\n",
        "\n",
        "# read the mesh file and find the number of points in it\n",
        "reader = vtk.vtkXMLUnstructuredGridReader()   # make a reader object\n",
        "reader.SetFileName(mesh_file)                 # read mesh file using reader object\n",
        "reader.Update()\n",
        "data_vtk = reader.GetOutput()\n",
        "n_points = data_vtk.GetNumberOfPoints()       # find the number of points in our mesh\n",
        "\n",
        "# define two empty numpy array in order to move the mesh coordinates in them\n",
        "x_vtk_mesh = np.zeros((n_points,1))\n",
        "y_vtk_mesh = np.zeros((n_points,1))\n",
        "\n",
        "VTKpoints = vtk.vtkPoints()\n",
        "\n",
        "# move the mesh coordinates in x_vtk_mesh & y_vtk_mesh  ( the mesh was 2D )\n",
        "for i in range(n_points):\n",
        "\tpt_iso  =  data_vtk.GetPoint(i)  # pt_iso has three dim that the first is x-coordinates, y-coordinates, and z-coordinates\n",
        "\tx_vtk_mesh[i] = pt_iso[0]\t\n",
        "\ty_vtk_mesh[i] = pt_iso[1]\n",
        "\tVTKpoints.InsertPoint(i, pt_iso[0], pt_iso[1], pt_iso[2])  \n",
        "point_data = vtk.vtkUnstructuredGrid()\n",
        "point_data.SetPoints(VTKpoints)\n",
        "\n",
        "x  = np.reshape(x_vtk_mesh , (np.size(x_vtk_mesh [:]),1))    # move x_vtk_mesh into x \n",
        "y  = np.reshape(y_vtk_mesh , (np.size(y_vtk_mesh [:]),1))    # move y_vtk_mesh into y\n",
        "\n",
        "print ('preparing the mesh coordinates ends and the number of points was:{}'.format(n_points))\n",
        "\n",
        "print ('\\n\\nLoading the inlet Boundry file and make the inlet boundry coordinates ready in variables xb_in & yb_in')\n",
        "\n",
        "# read the inlet boundry file and find the number of points in it\n",
        "reader = vtk.vtkUnstructuredGridReader()\n",
        "reader.SetFileName(bc_file_in)\n",
        "reader.Update()\n",
        "data_vtk = reader.GetOutput()\n",
        "n_points = data_vtk.GetNumberOfPoints()\n",
        "\n",
        "# define two empty numpy array in order to move the inlet boundry coordinates in them\n",
        "x_vtk_mesh = np.zeros((n_points,1))\n",
        "y_vtk_mesh = np.zeros((n_points,1))\n",
        "\n",
        "VTKpoints = vtk.vtkPoints()\n",
        "\n",
        "# move the inlet boundry coordinates in x_vtk_mesh & y_vtk_mesh\n",
        "for i in range(n_points):\n",
        "\tpt_iso  =  data_vtk.GetPoint(i)\n",
        "\tx_vtk_mesh[i] = pt_iso[0]\t\n",
        "\ty_vtk_mesh[i] = pt_iso[1]\n",
        "\tVTKpoints.InsertPoint(i, pt_iso[0], pt_iso[1], pt_iso[2])\n",
        "point_data = vtk.vtkUnstructuredGrid()\n",
        "point_data.SetPoints(VTKpoints)\n",
        "\n",
        "xb_in  = np.reshape(x_vtk_mesh , (np.size(x_vtk_mesh[:]),1))    # move x_vtk_mesh into xb_in\n",
        "yb_in  = np.reshape(y_vtk_mesh , (np.size(y_vtk_mesh[:]),1))    # move y_vtk_mesh into yb_in\n",
        "\n",
        "print ('preparing the inlet boundry coordinates ends and the number of points was:{}'.format(n_points))\n",
        "\n",
        "print ('\\n\\nLoading the wall Boundry file and make the wall boundry coordinates ready in variables xb_wall & yb_wall')\n",
        "\n",
        "# read the wall boundry file and find the number of points in it\n",
        "reader = vtk.vtkUnstructuredGridReader()\n",
        "reader.SetFileName(bc_file_wall)\n",
        "reader.Update()\n",
        "data_vtk = reader.GetOutput()\n",
        "n_pointsw = data_vtk.GetNumberOfPoints()\n",
        "\n",
        "# define two empty numpy array in order to move the wall boundry coordinates in them\n",
        "x_vtk_mesh = np.zeros((n_pointsw,1))\n",
        "y_vtk_mesh = np.zeros((n_pointsw,1))\n",
        "VTKpoints = vtk.vtkPoints()\n",
        "\n",
        "# move the wall boundry coordinates in x_vtk_mesh & y_vtk_mesh\n",
        "for i in range(n_pointsw):\n",
        "\tpt_iso  =  data_vtk.GetPoint(i)\n",
        "\tx_vtk_mesh[i] = pt_iso[0]\t\n",
        "\ty_vtk_mesh[i] = pt_iso[1]\n",
        "\tVTKpoints.InsertPoint(i, pt_iso[0], pt_iso[1], pt_iso[2])\n",
        "point_data = vtk.vtkUnstructuredGrid()\n",
        "point_data.SetPoints(VTKpoints)\n",
        "\n",
        "xb_wall  = np.reshape(x_vtk_mesh , (np.size(x_vtk_mesh [:]),1))   # move x_vtk_mesh into xb_wall\n",
        "yb_wall  = np.reshape(y_vtk_mesh , (np.size(y_vtk_mesh [:]),1))   # move y_vtk_mesh into xb_wall\n",
        "\n",
        "print ('preparing the wall boundry coordinates ends and the number of points was:{}'.format(n_pointsw))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading the mesh file and make x & y coordinates ready in variables x & y\n",
            "preparing the mesh coordinates ends and the number of points was:39238\n",
            "\n",
            "\n",
            "Loading the inlet Boundry file and make the inlet boundry coordinates ready in variables xb_in & yb_in\n",
            "preparing the inlet boundry coordinates ends and the number of points was:131\n",
            "\n",
            "\n",
            "Loading the wall Boundry file and make the wall boundry coordinates ready in variables xb_wall & yb_wall\n",
            "preparing the wall boundry coordinates ends and the number of points was:760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gv0SnqTvvGY"
      },
      "source": [
        "## Read the 5 sensors Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il6UAjzGVxFI",
        "outputId": "6ce8e1e3-390a-4d93-a6e3-cd9d31363aa6"
      },
      "source": [
        "#!!specify pts location here:         (sensor locations)\n",
        "x_data = [1., 1.2, 1.22, 1.31, 1.39 ] \n",
        "y_data =[0.15, 0.07, 0.22, 0.036, 0.26 ]\n",
        "z_data  = [0.,0.,0.,0.,0. ]\n",
        "\n",
        "x_data = np.asarray(x_data)  #convert to numpy \n",
        "y_data = np.asarray(y_data) #convert to numpy \n",
        "\n",
        "print ('Loading the file data (ground truth) to assign to our 5 sensors in variables data_vel_u (u direction) & data_vel_v(v direction)')\n",
        "reader = vtk.vtkXMLUnstructuredGridReader()\n",
        "reader.SetFileName(File_data)\n",
        "reader.Update()\n",
        "data_vtk = reader.GetOutput()\n",
        "n_points = data_vtk.GetNumberOfPoints()\n",
        "print ('\\nthe number of ground truth data in the file data was:' ,n_points)\n",
        "\n",
        "VTKpoints = vtk.vtkPoints()\n",
        "for i in range(len(x_data)): \n",
        "\tVTKpoints.InsertPoint(i, x_data[i] , y_data[i]  , z_data[i])\n",
        "point_data = vtk.vtkUnstructuredGrid()\n",
        "point_data.SetPoints(VTKpoints)\n",
        "probe = vtk.vtkProbeFilter()\n",
        "probe.SetInputData(point_data)\n",
        "probe.SetSourceData(data_vtk)\n",
        "probe.Update()\n",
        "array = probe.GetOutput().GetPointData().GetArray(fieldname)\n",
        "data_vel = VN.vtk_to_numpy(array)  # the value of u and v direction in the 5 sensor locations\n",
        "\n",
        "data_vel_u = data_vel[:,0]\n",
        "data_vel_v = data_vel[:,1]\n",
        "\n",
        "\n",
        "X_scale = 2.0 #The length of the  domain for locating the sensor was 2 now we have to divide the x coordinates by 2 to be like the actual mesh \n",
        "              #(need longer length for separation region)\n",
        "x_data = x_data / X_scale\n",
        "\n",
        "\n",
        "print('sensor coordinates x:{} y:{} '.format(x_data,y_data))\n",
        "print('Sensor value in directio of u: ',data_vel_u)\n",
        "print('Sensor value in directio of v: ',data_vel_v)\n",
        "\n",
        "# to be confident that these parameters are an array in one column\n",
        "x_data= x_data.reshape(-1, 1) #need to reshape to get 2D array\n",
        "y_data= y_data.reshape(-1, 1) #need to reshape to get 2D array\n",
        "data_vel_u= data_vel_u.reshape(-1, 1) #need to reshape to get 2D array\n",
        "data_vel_v= data_vel_v.reshape(-1, 1) #need to reshape to get 2D array"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the file data (ground truth) to assign to our 5 sensors in variables data_vel_u (u direction) & data_vel_v(v direction)\n",
            "\n",
            "the number of ground truth data in the file data was: 21140\n",
            "sensor coordinates x:[0.5   0.6   0.61  0.655 0.695] y:[0.15  0.07  0.22  0.036 0.26 ] \n",
            "Sensor value in directio of u:  [0.82861364 0.21744607 0.3266157  0.03768962 0.08355277]\n",
            "Sensor value in directio of v:  [ 1.0263980e-05 -1.6375961e-02  2.1537246e-02 -8.2011754e-03\n",
            "  9.3381843e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dcjiRQAY7N"
      },
      "source": [
        "## Define the wall Boundry conditions, and inlet boundry conditions, then reshape them to be prepared"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "505zJi4CVzS-",
        "outputId": "7e9ecc28-fd74-4db7-9281-7843b26e8fcf"
      },
      "source": [
        "# defining the boundry conditions\n",
        "U_BC_in = 0.5\n",
        "u_in_BC = (yb_in[:]) * ( 0.3 - yb_in[:] )  / 0.0225 * U_BC_in #parabolic  #inlet boundry condition in direction u\n",
        "v_in_BC = np.linspace(0., 0., n_points)                                   #inlet boundry condition in direction v\n",
        "u_wall_BC = np.linspace(0., 0., n_pointsw)                                #wall boundry condition in direction u\n",
        "v_wall_BC = np.linspace(0., 0., n_pointsw)                                #wall boundry condition in direction v\n",
        "\n",
        "# to be confident that these parameters are an array in one column\n",
        "xb_wall= xb_wall.reshape(-1, 1) #need to reshape to get 2D array\n",
        "yb_wall= yb_wall.reshape(-1, 1) #need to reshape to get 2D array\n",
        "u_wall_BC= u_wall_BC.reshape(-1, 1) #need to reshape to get 2D array\n",
        "v_wall_BC= v_wall_BC.reshape(-1, 1) #need to reshape to get 2D array\n",
        "xb_in= xb_in.reshape(-1, 1) #need to reshape to get 2D array\n",
        "yb_in= yb_in.reshape(-1, 1) #need to reshape to get 2D array\n",
        "u_in_BC= u_in_BC.reshape(-1, 1) #need to reshape to get 2D array\n",
        "v_in_BC= v_in_BC.reshape(-1, 1) #need to reshape to get 2D array\n",
        "print('shape of xb_wall (x coordinates in the wall boundry)',xb_wall.shape)\n",
        "print('shape of yb_wall (y coordinates in the wall boundry)',yb_wall.shape)\n",
        "print('shape of u_wall_BC (value of the wall boundry in direction of u)',u_wall_BC.shape)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of xb_wall (x coordinates in the wall boundry) (760, 1)\n",
            "shape of yb_wall (y coordinates in the wall boundry) (760, 1)\n",
            "shape of u_wall_BC (value of the wall boundry in direction of u) (760, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4rCWP0G4a1b"
      },
      "source": [
        "# Define some properties for the network and training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJBBRR8rV3Ll"
      },
      "source": [
        "Flag_batch = True \n",
        "Flag_BC_exact = False \n",
        "Flag_pretrain = False # True #If true reads the nets from last run (for train or use pretrained network)\n",
        "path = \"Results/\"\n",
        "batchsize = 256 \n",
        "epochs  = 100\n",
        "\n",
        "path = \"/content/drive/My Drive/stenosis/Results/\"\t\t# saving network\n",
        "\n",
        "if not os.path.isfile(path+\"loss_siren.xlsx\"):\n",
        "\theaders= ['Loss_eqn','Loss_BC','Loss_Data','Loss_total','Time']\n",
        "\tworkbook_name = path+'loss_siren.xlsx'\n",
        "\twb = Workbook()\n",
        "\tpage = wb.active\n",
        "\tpage.title = 'Siren-Based-Stenosis'\n",
        "\tpage.append(headers) # write the headers to the first line\n",
        "\twb.save(filename=workbook_name)\n",
        "\n",
        "learning_rate = 1e-5 \n",
        "Flag_schedule = True #If true change the learning rate during training\n",
        "if (Flag_schedule):\n",
        "\tlearning_rate = 5e-4 #starting learning rate\n",
        "\tstep_epoch = 500 #1000\n",
        "\tdecay_rate = 0.1 # 0.1\n",
        "\n",
        "# parameteres of the equation\n",
        "Diff = 0.001\n",
        "rho = 1.\n",
        "T = 0.5 #total duraction\n",
        "#nPt_time = 50 #number of time-steps\n",
        "\n",
        "Lambda_BC  = 20.\n",
        "nPt = 130  \n",
        "\n",
        "path = \"/content/drive/My Drive/stenosis/Results/\"\t\t# saving network\n",
        "\n",
        "\n",
        "\n",
        "Flag_x_length = True #if True scales the eqn such that the length of the domain is = X_scale\n",
        "X_scale = 2.0 #The length of the  domain (need longer length for separation region)\n",
        "Y_scale = 1.0 \n",
        "U_scale = 1.0\n",
        "\n",
        "Lambda_data = 1."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP20VNHO3qRF"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLOhE-wkV6x2",
        "outputId": "5815b379-90ef-4c25-fc9a-7acc121bc2ac"
      },
      "source": [
        "\n",
        "net = geo_train(device,x,y,xb_wall,yb_wall,u_wall_BC,v_wall_BC,x_data,y_data,data_vel_u,data_vel_v\n",
        "          ,batchsize,learning_rate,epochs,path,Flag_batch,Diff,rho,Flag_BC_exact,Lambda_BC,nPt,T,xb_in,yb_in,u_in_BC,v_in_BC )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading (pretrain) functions first...\n",
            "Train Epoch: 0 [0/39238 (0%)]\tLoss eqn 0.0013407723 Loss BC 0.00002032 Loss data 0.00005658\n",
            "Train Epoch: 0 [10240/39238 (26%)]\tLoss eqn 0.0021873466 Loss BC 0.00002104 Loss data 0.00021859\n",
            "Train Epoch: 0 [20480/39238 (52%)]\tLoss eqn 0.0013058684 Loss BC 0.00001228 Loss data 0.00002691\n",
            "Train Epoch: 0 [30720/39238 (78%)]\tLoss eqn 0.0024156671 Loss BC 0.00002250 Loss data 0.00003068\n",
            "*****Total avg Loss : Loss eqn 0.0029763693 Loss BC 0.0000530294 Loss data 0.0001567606 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 1 [0/39238 (0%)]\tLoss eqn 0.0005642610 Loss BC 0.00001444 Loss data 0.00003347\n",
            "Train Epoch: 1 [10240/39238 (26%)]\tLoss eqn 0.0008453216 Loss BC 0.00001754 Loss data 0.00006015\n",
            "Train Epoch: 1 [20480/39238 (52%)]\tLoss eqn 0.0051190145 Loss BC 0.00002890 Loss data 0.00015544\n",
            "Train Epoch: 1 [30720/39238 (78%)]\tLoss eqn 0.0017030055 Loss BC 0.00002019 Loss data 0.00002493\n",
            "*****Total avg Loss : Loss eqn 0.0017017551 Loss BC 0.0000235869 Loss data 0.0000489289 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 2 [0/39238 (0%)]\tLoss eqn 0.0006946436 Loss BC 0.00001220 Loss data 0.00001221\n",
            "Train Epoch: 2 [10240/39238 (26%)]\tLoss eqn 0.0011824500 Loss BC 0.00001706 Loss data 0.00004396\n",
            "Train Epoch: 2 [20480/39238 (52%)]\tLoss eqn 0.0018250635 Loss BC 0.00001271 Loss data 0.00001441\n",
            "Train Epoch: 2 [30720/39238 (78%)]\tLoss eqn 0.0030900864 Loss BC 0.00008749 Loss data 0.00014802\n",
            "*****Total avg Loss : Loss eqn 0.0024114321 Loss BC 0.0000351560 Loss data 0.0000763810 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 3 [0/39238 (0%)]\tLoss eqn 0.0011405712 Loss BC 0.00004824 Loss data 0.00015303\n",
            "Train Epoch: 3 [10240/39238 (26%)]\tLoss eqn 0.0018473384 Loss BC 0.00003029 Loss data 0.00009321\n",
            "Train Epoch: 3 [20480/39238 (52%)]\tLoss eqn 0.0013804119 Loss BC 0.00002149 Loss data 0.00002066\n",
            "Train Epoch: 3 [30720/39238 (78%)]\tLoss eqn 0.0029256751 Loss BC 0.00002509 Loss data 0.00004912\n",
            "*****Total avg Loss : Loss eqn 0.0020891374 Loss BC 0.0000304753 Loss data 0.0000633895 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 4 [0/39238 (0%)]\tLoss eqn 0.0085112024 Loss BC 0.00004263 Loss data 0.00021138\n",
            "Train Epoch: 4 [10240/39238 (26%)]\tLoss eqn 0.0037048138 Loss BC 0.00003969 Loss data 0.00053175\n",
            "Train Epoch: 4 [20480/39238 (52%)]\tLoss eqn 0.0011539343 Loss BC 0.00001370 Loss data 0.00005375\n",
            "Train Epoch: 4 [30720/39238 (78%)]\tLoss eqn 0.0008322605 Loss BC 0.00001400 Loss data 0.00003895\n",
            "*****Total avg Loss : Loss eqn 0.0027224580 Loss BC 0.0000346007 Loss data 0.0001185562 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 5 [0/39238 (0%)]\tLoss eqn 0.0005223153 Loss BC 0.00001311 Loss data 0.00001992\n",
            "Train Epoch: 5 [10240/39238 (26%)]\tLoss eqn 0.0016055695 Loss BC 0.00004018 Loss data 0.00005057\n",
            "Train Epoch: 5 [20480/39238 (52%)]\tLoss eqn 0.0021343739 Loss BC 0.00002520 Loss data 0.00007105\n",
            "Train Epoch: 5 [30720/39238 (78%)]\tLoss eqn 0.0009840193 Loss BC 0.00001915 Loss data 0.00003212\n",
            "*****Total avg Loss : Loss eqn 0.0015822098 Loss BC 0.0000227408 Loss data 0.0000417978 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 6 [0/39238 (0%)]\tLoss eqn 0.0022554332 Loss BC 0.00001348 Loss data 0.00004311\n",
            "Train Epoch: 6 [10240/39238 (26%)]\tLoss eqn 0.0034917230 Loss BC 0.00006449 Loss data 0.00008969\n",
            "Train Epoch: 6 [20480/39238 (52%)]\tLoss eqn 0.0097356867 Loss BC 0.00002860 Loss data 0.00009303\n",
            "Train Epoch: 6 [30720/39238 (78%)]\tLoss eqn 0.0013849826 Loss BC 0.00002333 Loss data 0.00002952\n",
            "*****Total avg Loss : Loss eqn 0.0024840017 Loss BC 0.0000326223 Loss data 0.0000849043 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 7 [0/39238 (0%)]\tLoss eqn 0.0010599557 Loss BC 0.00000718 Loss data 0.00002255\n",
            "Train Epoch: 7 [10240/39238 (26%)]\tLoss eqn 0.0074876803 Loss BC 0.00011337 Loss data 0.00032104\n",
            "Train Epoch: 7 [20480/39238 (52%)]\tLoss eqn 0.0013029471 Loss BC 0.00002010 Loss data 0.00007113\n",
            "Train Epoch: 7 [30720/39238 (78%)]\tLoss eqn 0.0007955987 Loss BC 0.00000959 Loss data 0.00002359\n",
            "*****Total avg Loss : Loss eqn 0.0028192147 Loss BC 0.0000411042 Loss data 0.0001139316 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 8 [0/39238 (0%)]\tLoss eqn 0.0006449853 Loss BC 0.00001364 Loss data 0.00001975\n",
            "Train Epoch: 8 [10240/39238 (26%)]\tLoss eqn 0.0008940317 Loss BC 0.00001327 Loss data 0.00002286\n",
            "Train Epoch: 8 [20480/39238 (52%)]\tLoss eqn 0.0032828362 Loss BC 0.00002033 Loss data 0.00004203\n",
            "Train Epoch: 8 [30720/39238 (78%)]\tLoss eqn 0.0031759026 Loss BC 0.00004610 Loss data 0.00005485\n",
            "*****Total avg Loss : Loss eqn 0.0027639484 Loss BC 0.0000369002 Loss data 0.0000913540 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 9 [0/39238 (0%)]\tLoss eqn 0.0030128430 Loss BC 0.00004573 Loss data 0.00024083\n",
            "Train Epoch: 9 [10240/39238 (26%)]\tLoss eqn 0.0011726059 Loss BC 0.00002573 Loss data 0.00005065\n",
            "Train Epoch: 9 [20480/39238 (52%)]\tLoss eqn 0.0007984467 Loss BC 0.00001620 Loss data 0.00002085\n",
            "Train Epoch: 9 [30720/39238 (78%)]\tLoss eqn 0.0079427976 Loss BC 0.00020026 Loss data 0.00051252\n",
            "*****Total avg Loss : Loss eqn 0.0033495887 Loss BC 0.0000542340 Loss data 0.0001460133 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 10 [0/39238 (0%)]\tLoss eqn 0.0032867235 Loss BC 0.00002790 Loss data 0.00023043\n",
            "Train Epoch: 10 [10240/39238 (26%)]\tLoss eqn 0.0008206924 Loss BC 0.00000852 Loss data 0.00004399\n",
            "Train Epoch: 10 [20480/39238 (52%)]\tLoss eqn 0.0008183140 Loss BC 0.00001269 Loss data 0.00002180\n",
            "Train Epoch: 10 [30720/39238 (78%)]\tLoss eqn 0.0025279124 Loss BC 0.00002849 Loss data 0.00004725\n",
            "*****Total avg Loss : Loss eqn 0.0013737478 Loss BC 0.0000185123 Loss data 0.0000526428 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 11 [0/39238 (0%)]\tLoss eqn 0.0015023462 Loss BC 0.00002122 Loss data 0.00004112\n",
            "Train Epoch: 11 [10240/39238 (26%)]\tLoss eqn 0.0015159110 Loss BC 0.00003084 Loss data 0.00012052\n",
            "Train Epoch: 11 [20480/39238 (52%)]\tLoss eqn 0.0010116511 Loss BC 0.00001452 Loss data 0.00004096\n",
            "Train Epoch: 11 [30720/39238 (78%)]\tLoss eqn 0.0017744955 Loss BC 0.00002229 Loss data 0.00005193\n",
            "*****Total avg Loss : Loss eqn 0.0018329771 Loss BC 0.0000241075 Loss data 0.0000484692 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 12 [0/39238 (0%)]\tLoss eqn 0.0051143998 Loss BC 0.00006550 Loss data 0.00027137\n",
            "Train Epoch: 12 [10240/39238 (26%)]\tLoss eqn 0.0020608734 Loss BC 0.00003518 Loss data 0.00006820\n",
            "Train Epoch: 12 [20480/39238 (52%)]\tLoss eqn 0.0011585960 Loss BC 0.00001661 Loss data 0.00006207\n",
            "Train Epoch: 12 [30720/39238 (78%)]\tLoss eqn 0.0043403264 Loss BC 0.00004046 Loss data 0.00014521\n",
            "*****Total avg Loss : Loss eqn 0.0022022917 Loss BC 0.0000306845 Loss data 0.0000751903 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 13 [0/39238 (0%)]\tLoss eqn 0.0015066369 Loss BC 0.00002189 Loss data 0.00007988\n",
            "Train Epoch: 13 [10240/39238 (26%)]\tLoss eqn 0.0020209686 Loss BC 0.00002576 Loss data 0.00011174\n",
            "Train Epoch: 13 [20480/39238 (52%)]\tLoss eqn 0.0030420327 Loss BC 0.00002342 Loss data 0.00010573\n",
            "Train Epoch: 13 [30720/39238 (78%)]\tLoss eqn 0.0055520278 Loss BC 0.00008248 Loss data 0.00025281\n",
            "*****Total avg Loss : Loss eqn 0.0033035742 Loss BC 0.0000441361 Loss data 0.0001227559 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 14 [0/39238 (0%)]\tLoss eqn 0.0014530887 Loss BC 0.00002648 Loss data 0.00005475\n",
            "Train Epoch: 14 [10240/39238 (26%)]\tLoss eqn 0.0009073165 Loss BC 0.00001266 Loss data 0.00004182\n",
            "Train Epoch: 14 [20480/39238 (52%)]\tLoss eqn 0.0012438267 Loss BC 0.00002367 Loss data 0.00002740\n",
            "Train Epoch: 14 [30720/39238 (78%)]\tLoss eqn 0.0014485517 Loss BC 0.00001948 Loss data 0.00006303\n",
            "*****Total avg Loss : Loss eqn 0.0012556536 Loss BC 0.0000191203 Loss data 0.0000395442 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 15 [0/39238 (0%)]\tLoss eqn 0.0022939227 Loss BC 0.00003765 Loss data 0.00010862\n",
            "Train Epoch: 15 [10240/39238 (26%)]\tLoss eqn 0.0026440881 Loss BC 0.00002594 Loss data 0.00012198\n",
            "Train Epoch: 15 [20480/39238 (52%)]\tLoss eqn 0.0007310693 Loss BC 0.00001127 Loss data 0.00002069\n",
            "Train Epoch: 15 [30720/39238 (78%)]\tLoss eqn 0.0021310786 Loss BC 0.00001800 Loss data 0.00009406\n",
            "*****Total avg Loss : Loss eqn 0.0023956650 Loss BC 0.0000323961 Loss data 0.0000727283 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 16 [0/39238 (0%)]\tLoss eqn 0.0028947627 Loss BC 0.00004598 Loss data 0.00007044\n",
            "Train Epoch: 16 [10240/39238 (26%)]\tLoss eqn 0.0031421678 Loss BC 0.00001914 Loss data 0.00002455\n",
            "Train Epoch: 16 [20480/39238 (52%)]\tLoss eqn 0.0038996798 Loss BC 0.00004937 Loss data 0.00024358\n",
            "Train Epoch: 16 [30720/39238 (78%)]\tLoss eqn 0.0006714356 Loss BC 0.00000963 Loss data 0.00003089\n",
            "*****Total avg Loss : Loss eqn 0.0024182987 Loss BC 0.0000283489 Loss data 0.0000841160 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 17 [0/39238 (0%)]\tLoss eqn 0.0014627312 Loss BC 0.00003172 Loss data 0.00002591\n",
            "Train Epoch: 17 [10240/39238 (26%)]\tLoss eqn 0.0054677334 Loss BC 0.00005334 Loss data 0.00009831\n",
            "Train Epoch: 17 [20480/39238 (52%)]\tLoss eqn 0.0038017917 Loss BC 0.00005093 Loss data 0.00004723\n",
            "Train Epoch: 17 [30720/39238 (78%)]\tLoss eqn 0.0030554989 Loss BC 0.00004127 Loss data 0.00013320\n",
            "*****Total avg Loss : Loss eqn 0.0024355801 Loss BC 0.0000365442 Loss data 0.0000832989 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 18 [0/39238 (0%)]\tLoss eqn 0.0010829641 Loss BC 0.00001631 Loss data 0.00002844\n",
            "Train Epoch: 18 [10240/39238 (26%)]\tLoss eqn 0.0026862300 Loss BC 0.00002941 Loss data 0.00006471\n",
            "Train Epoch: 18 [20480/39238 (52%)]\tLoss eqn 0.0013562152 Loss BC 0.00001543 Loss data 0.00003083\n",
            "Train Epoch: 18 [30720/39238 (78%)]\tLoss eqn 0.0011192271 Loss BC 0.00001220 Loss data 0.00002063\n",
            "*****Total avg Loss : Loss eqn 0.0016439931 Loss BC 0.0000228027 Loss data 0.0000451427 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 19 [0/39238 (0%)]\tLoss eqn 0.0053888028 Loss BC 0.00011235 Loss data 0.00013807\n",
            "Train Epoch: 19 [10240/39238 (26%)]\tLoss eqn 0.0033044184 Loss BC 0.00003774 Loss data 0.00012881\n",
            "Train Epoch: 19 [20480/39238 (52%)]\tLoss eqn 0.0010000733 Loss BC 0.00003036 Loss data 0.00003085\n",
            "Train Epoch: 19 [30720/39238 (78%)]\tLoss eqn 0.0026163128 Loss BC 0.00004334 Loss data 0.00008464\n",
            "*****Total avg Loss : Loss eqn 0.0037372343 Loss BC 0.0000507209 Loss data 0.0001421833 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 20 [0/39238 (0%)]\tLoss eqn 0.0018431027 Loss BC 0.00002781 Loss data 0.00005402\n",
            "Train Epoch: 20 [10240/39238 (26%)]\tLoss eqn 0.0054541347 Loss BC 0.00012676 Loss data 0.00020415\n",
            "Train Epoch: 20 [20480/39238 (52%)]\tLoss eqn 0.0022742297 Loss BC 0.00001981 Loss data 0.00007107\n",
            "Train Epoch: 20 [30720/39238 (78%)]\tLoss eqn 0.0005254841 Loss BC 0.00001012 Loss data 0.00004132\n",
            "*****Total avg Loss : Loss eqn 0.0023902906 Loss BC 0.0000387345 Loss data 0.0001079202 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 21 [0/39238 (0%)]\tLoss eqn 0.0015214677 Loss BC 0.00001199 Loss data 0.00006563\n",
            "Train Epoch: 21 [10240/39238 (26%)]\tLoss eqn 0.0019540577 Loss BC 0.00001767 Loss data 0.00008111\n",
            "Train Epoch: 21 [20480/39238 (52%)]\tLoss eqn 0.0035053638 Loss BC 0.00002392 Loss data 0.00001999\n",
            "Train Epoch: 21 [30720/39238 (78%)]\tLoss eqn 0.0023202589 Loss BC 0.00004478 Loss data 0.00003946\n",
            "*****Total avg Loss : Loss eqn 0.0022312512 Loss BC 0.0000306554 Loss data 0.0000680175 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 22 [0/39238 (0%)]\tLoss eqn 0.0013297895 Loss BC 0.00001358 Loss data 0.00000990\n",
            "Train Epoch: 22 [10240/39238 (26%)]\tLoss eqn 0.0031046525 Loss BC 0.00000868 Loss data 0.00002862\n",
            "Train Epoch: 22 [20480/39238 (52%)]\tLoss eqn 0.0013489322 Loss BC 0.00001243 Loss data 0.00005049\n",
            "Train Epoch: 22 [30720/39238 (78%)]\tLoss eqn 0.0019020322 Loss BC 0.00002442 Loss data 0.00007322\n",
            "*****Total avg Loss : Loss eqn 0.0018671985 Loss BC 0.0000265879 Loss data 0.0000545669 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 23 [0/39238 (0%)]\tLoss eqn 0.0011040743 Loss BC 0.00001183 Loss data 0.00004416\n",
            "Train Epoch: 23 [10240/39238 (26%)]\tLoss eqn 0.0042343154 Loss BC 0.00016246 Loss data 0.00033963\n",
            "Train Epoch: 23 [20480/39238 (52%)]\tLoss eqn 0.0049631279 Loss BC 0.00003210 Loss data 0.00019473\n",
            "Train Epoch: 23 [30720/39238 (78%)]\tLoss eqn 0.0010453102 Loss BC 0.00001562 Loss data 0.00001878\n",
            "*****Total avg Loss : Loss eqn 0.0024807737 Loss BC 0.0000413534 Loss data 0.0000946832 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 24 [0/39238 (0%)]\tLoss eqn 0.0009428072 Loss BC 0.00001296 Loss data 0.00004867\n",
            "Train Epoch: 24 [10240/39238 (26%)]\tLoss eqn 0.0017431329 Loss BC 0.00001383 Loss data 0.00008615\n",
            "Train Epoch: 24 [20480/39238 (52%)]\tLoss eqn 0.0043026544 Loss BC 0.00004131 Loss data 0.00013243\n",
            "Train Epoch: 24 [30720/39238 (78%)]\tLoss eqn 0.0013721336 Loss BC 0.00001465 Loss data 0.00001987\n",
            "*****Total avg Loss : Loss eqn 0.0018703092 Loss BC 0.0000268087 Loss data 0.0000549876 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 25 [0/39238 (0%)]\tLoss eqn 0.0041626720 Loss BC 0.00002239 Loss data 0.00004342\n",
            "Train Epoch: 25 [10240/39238 (26%)]\tLoss eqn 0.0024260471 Loss BC 0.00002839 Loss data 0.00005556\n",
            "Train Epoch: 25 [20480/39238 (52%)]\tLoss eqn 0.0010799839 Loss BC 0.00001408 Loss data 0.00002729\n",
            "Train Epoch: 25 [30720/39238 (78%)]\tLoss eqn 0.0038969265 Loss BC 0.00003517 Loss data 0.00007019\n",
            "*****Total avg Loss : Loss eqn 0.0020306939 Loss BC 0.0000274014 Loss data 0.0000600668 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 26 [0/39238 (0%)]\tLoss eqn 0.0015105708 Loss BC 0.00002245 Loss data 0.00008945\n",
            "Train Epoch: 26 [10240/39238 (26%)]\tLoss eqn 0.0029579117 Loss BC 0.00002387 Loss data 0.00009773\n",
            "Train Epoch: 26 [20480/39238 (52%)]\tLoss eqn 0.0021873422 Loss BC 0.00002373 Loss data 0.00011477\n",
            "Train Epoch: 26 [30720/39238 (78%)]\tLoss eqn 0.0013863982 Loss BC 0.00001513 Loss data 0.00006025\n",
            "*****Total avg Loss : Loss eqn 0.0018288611 Loss BC 0.0000264795 Loss data 0.0000582285 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 27 [0/39238 (0%)]\tLoss eqn 0.0007942918 Loss BC 0.00001794 Loss data 0.00003805\n",
            "Train Epoch: 27 [10240/39238 (26%)]\tLoss eqn 0.0027627517 Loss BC 0.00002414 Loss data 0.00009828\n",
            "Train Epoch: 27 [20480/39238 (52%)]\tLoss eqn 0.0067422637 Loss BC 0.00005446 Loss data 0.00060303\n",
            "Train Epoch: 27 [30720/39238 (78%)]\tLoss eqn 0.0021925238 Loss BC 0.00001506 Loss data 0.00004804\n",
            "*****Total avg Loss : Loss eqn 0.0030432236 Loss BC 0.0000406213 Loss data 0.0001362081 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 28 [0/39238 (0%)]\tLoss eqn 0.0009326406 Loss BC 0.00002316 Loss data 0.00004832\n",
            "Train Epoch: 28 [10240/39238 (26%)]\tLoss eqn 0.0020115969 Loss BC 0.00004503 Loss data 0.00002898\n",
            "Train Epoch: 28 [20480/39238 (52%)]\tLoss eqn 0.0010368933 Loss BC 0.00003546 Loss data 0.00004020\n",
            "Train Epoch: 28 [30720/39238 (78%)]\tLoss eqn 0.0017267978 Loss BC 0.00002194 Loss data 0.00007951\n",
            "*****Total avg Loss : Loss eqn 0.0015530771 Loss BC 0.0000226857 Loss data 0.0000447861 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 29 [0/39238 (0%)]\tLoss eqn 0.0028663867 Loss BC 0.00002860 Loss data 0.00007177\n",
            "Train Epoch: 29 [10240/39238 (26%)]\tLoss eqn 0.0014405710 Loss BC 0.00002492 Loss data 0.00005983\n",
            "Train Epoch: 29 [20480/39238 (52%)]\tLoss eqn 0.0029364019 Loss BC 0.00005188 Loss data 0.00018229\n",
            "Train Epoch: 29 [30720/39238 (78%)]\tLoss eqn 0.0042309095 Loss BC 0.00003590 Loss data 0.00006162\n",
            "*****Total avg Loss : Loss eqn 0.0027085757 Loss BC 0.0000344739 Loss data 0.0000978137 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 30 [0/39238 (0%)]\tLoss eqn 0.0011054446 Loss BC 0.00001322 Loss data 0.00005043\n",
            "Train Epoch: 30 [10240/39238 (26%)]\tLoss eqn 0.0009107228 Loss BC 0.00000914 Loss data 0.00002554\n",
            "Train Epoch: 30 [20480/39238 (52%)]\tLoss eqn 0.0029678911 Loss BC 0.00001393 Loss data 0.00001181\n",
            "Train Epoch: 30 [30720/39238 (78%)]\tLoss eqn 0.0014367872 Loss BC 0.00002882 Loss data 0.00002174\n",
            "*****Total avg Loss : Loss eqn 0.0017788725 Loss BC 0.0000238821 Loss data 0.0000520796 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 31 [0/39238 (0%)]\tLoss eqn 0.0023291677 Loss BC 0.00004190 Loss data 0.00012749\n",
            "Train Epoch: 31 [10240/39238 (26%)]\tLoss eqn 0.0011098271 Loss BC 0.00002919 Loss data 0.00003461\n",
            "Train Epoch: 31 [20480/39238 (52%)]\tLoss eqn 0.0020135122 Loss BC 0.00004451 Loss data 0.00009072\n",
            "Train Epoch: 31 [30720/39238 (78%)]\tLoss eqn 0.0010055774 Loss BC 0.00000969 Loss data 0.00006463\n",
            "*****Total avg Loss : Loss eqn 0.0022479240 Loss BC 0.0000304433 Loss data 0.0000798819 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 32 [0/39238 (0%)]\tLoss eqn 0.0011265458 Loss BC 0.00002046 Loss data 0.00004969\n",
            "Train Epoch: 32 [10240/39238 (26%)]\tLoss eqn 0.0007203061 Loss BC 0.00001239 Loss data 0.00001870\n",
            "Train Epoch: 32 [20480/39238 (52%)]\tLoss eqn 0.0023341915 Loss BC 0.00002184 Loss data 0.00007276\n",
            "Train Epoch: 32 [30720/39238 (78%)]\tLoss eqn 0.0020377631 Loss BC 0.00002286 Loss data 0.00002446\n",
            "*****Total avg Loss : Loss eqn 0.0020135245 Loss BC 0.0000264629 Loss data 0.0000602474 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 33 [0/39238 (0%)]\tLoss eqn 0.0026601504 Loss BC 0.00003766 Loss data 0.00010179\n",
            "Train Epoch: 33 [10240/39238 (26%)]\tLoss eqn 0.0023425622 Loss BC 0.00007411 Loss data 0.00012058\n",
            "Train Epoch: 33 [20480/39238 (52%)]\tLoss eqn 0.0018305883 Loss BC 0.00003301 Loss data 0.00004867\n",
            "Train Epoch: 33 [30720/39238 (78%)]\tLoss eqn 0.0026063584 Loss BC 0.00003706 Loss data 0.00003532\n",
            "*****Total avg Loss : Loss eqn 0.0024990377 Loss BC 0.0000326011 Loss data 0.0000858472 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 34 [0/39238 (0%)]\tLoss eqn 0.0027351899 Loss BC 0.00002425 Loss data 0.00009303\n",
            "Train Epoch: 34 [10240/39238 (26%)]\tLoss eqn 0.0027455187 Loss BC 0.00004109 Loss data 0.00006787\n",
            "Train Epoch: 34 [20480/39238 (52%)]\tLoss eqn 0.0016749608 Loss BC 0.00002359 Loss data 0.00003779\n",
            "Train Epoch: 34 [30720/39238 (78%)]\tLoss eqn 0.0031910983 Loss BC 0.00003451 Loss data 0.00017784\n",
            "*****Total avg Loss : Loss eqn 0.0021998778 Loss BC 0.0000316527 Loss data 0.0000692885 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 35 [0/39238 (0%)]\tLoss eqn 0.0012075992 Loss BC 0.00003350 Loss data 0.00004904\n",
            "Train Epoch: 35 [10240/39238 (26%)]\tLoss eqn 0.0010084783 Loss BC 0.00000922 Loss data 0.00003459\n",
            "Train Epoch: 35 [20480/39238 (52%)]\tLoss eqn 0.0022330077 Loss BC 0.00002343 Loss data 0.00007134\n",
            "Train Epoch: 35 [30720/39238 (78%)]\tLoss eqn 0.0009421250 Loss BC 0.00001535 Loss data 0.00003124\n",
            "*****Total avg Loss : Loss eqn 0.0012663909 Loss BC 0.0000164689 Loss data 0.0000376801 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 36 [0/39238 (0%)]\tLoss eqn 0.0014262229 Loss BC 0.00002227 Loss data 0.00005755\n",
            "Train Epoch: 36 [10240/39238 (26%)]\tLoss eqn 0.0015294217 Loss BC 0.00003932 Loss data 0.00007424\n",
            "Train Epoch: 36 [20480/39238 (52%)]\tLoss eqn 0.0010948447 Loss BC 0.00002885 Loss data 0.00004739\n",
            "Train Epoch: 36 [30720/39238 (78%)]\tLoss eqn 0.0012716076 Loss BC 0.00003509 Loss data 0.00004456\n",
            "*****Total avg Loss : Loss eqn 0.0018490865 Loss BC 0.0000262054 Loss data 0.0000564865 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 37 [0/39238 (0%)]\tLoss eqn 0.0015733294 Loss BC 0.00001674 Loss data 0.00001441\n",
            "Train Epoch: 37 [10240/39238 (26%)]\tLoss eqn 0.0039933310 Loss BC 0.00002239 Loss data 0.00007920\n",
            "Train Epoch: 37 [20480/39238 (52%)]\tLoss eqn 0.0066620377 Loss BC 0.00006726 Loss data 0.00019380\n",
            "Train Epoch: 37 [30720/39238 (78%)]\tLoss eqn 0.0007720586 Loss BC 0.00001039 Loss data 0.00005069\n",
            "*****Total avg Loss : Loss eqn 0.0023168414 Loss BC 0.0000305498 Loss data 0.0000773059 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 38 [0/39238 (0%)]\tLoss eqn 0.0015310349 Loss BC 0.00002484 Loss data 0.00006861\n",
            "Train Epoch: 38 [10240/39238 (26%)]\tLoss eqn 0.0010905070 Loss BC 0.00001896 Loss data 0.00002106\n",
            "Train Epoch: 38 [20480/39238 (52%)]\tLoss eqn 0.0027297097 Loss BC 0.00002647 Loss data 0.00002453\n",
            "Train Epoch: 38 [30720/39238 (78%)]\tLoss eqn 0.0013222462 Loss BC 0.00005022 Loss data 0.00006450\n",
            "*****Total avg Loss : Loss eqn 0.0017049967 Loss BC 0.0000234027 Loss data 0.0000499767 ****\n",
            "learning rate is  0.0005\n",
            "Train Epoch: 39 [0/39238 (0%)]\tLoss eqn 0.0014017753 Loss BC 0.00001918 Loss data 0.00005887\n",
            "Train Epoch: 39 [10240/39238 (26%)]\tLoss eqn 0.0021474969 Loss BC 0.00004048 Loss data 0.00005448\n",
            "Train Epoch: 39 [20480/39238 (52%)]\tLoss eqn 0.0014279652 Loss BC 0.00001884 Loss data 0.00003433\n",
            "Train Epoch: 39 [30720/39238 (78%)]\tLoss eqn 0.0014754431 Loss BC 0.00002517 Loss data 0.00005555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjXa98NyQhFH"
      },
      "source": [
        "# Just run this part if you want to check the outputs after the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8E9FspQ6FTB"
      },
      "source": [
        "x_result = torch.Tensor(x).to(device)\n",
        "y_result = torch.Tensor(y).to(device)\n",
        "net_in = torch.cat((x_result.requires_grad_(),y_result.requires_grad_()),1)\n",
        "outneti = net(net_in)  #evaluate model (runs out of memory for large GPU problems!)\n",
        "output_u = outneti[:,0]\n",
        "output_v = outneti[:,1]  #evaluate model\n",
        "output_p = outneti[:,2] \n",
        "output_u = output_u.cpu().data.numpy() #need to convert to cpu before converting to numpy\n",
        "output_v = output_v.cpu().data.numpy()\n",
        "output_p = output_p.cpu().data.numpy()\n",
        "npoints = len(x)\n",
        "z = np.zeros((npoints,1))\n",
        "u = output_u\n",
        "v = output_v\n",
        "vz = np.zeros((len(v),1))\n",
        "p = output_p\n",
        "v=np.float64(v)\n",
        "u=np.float64(u)\n",
        "pointsToVTK(path+\"./output_siren\", x, y, z, data = {\"velocity\" : (np.array(u),np.array(v),vz), \"pressure\" : p})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}